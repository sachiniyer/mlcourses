#+TAGS: CIL


* Whatâ€™s Hidden in a Randomly Weighted Neural Network?
** Review
*** Info
*Arxiv:* https://arxiv.org/pdf/1911.13299
*Authors:* Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, Mohammad Rastegari
*Date:* Mar 2020
*** Problem Being Solved?
There does not seem to be a specific objective to the paper, and instead the goal seems to be to understand how the "optimization and initialization of neural networks" can affect training and performance.
*** What is an overview of the method?
By using a randomly initialized set of weights in a predefined architecture like Resnet-50, the algorithm masks the weights, so that a certain subnetwork emerges. That mask is learned according to different tasks (using the edge-popup method) such that the masking results in a randomly initialized subnetwork with good performance on the given task. New subnetworks are generated for new tasks.
*** What are the metrics of success?
The metrics of success are the performance of the network/algorithm over varying tasks. Specifically, they judge the accuracy of the model on CIFAR-10/ImageNet and show high performance.
*** What are the key innovations over prior work?
They show that you don't actually need to train the weights of the network, and that randomly initialized networks with masking are enough to do prediction with. Additionally, they show a mask is learnable, and can provide accuracy within the network. Lastly, they also show this as a method of compression where a small neural network with random weights can actually provide accurate performance.
*** What are the key results?
- Accuracy on CIFAR-10 reaches ~87%
- Accuracy on ImageNet reaches ~69%
Additionally, they show that variance in the weights matters, with a sweet spot of 50% (where the most subnetwork permutations of the overall network exists)
*** How might this work have long term impact?
By providing insight into how much architecture choice and how random initialization of weights is enough, this work provides useful insights into how those factors may affect other neural networks (this doesn't seem like a method that is put into practice?).
** Questions
1. I don't fully understand why the scaled initialization method matters. Why does scaling to normal actually result in better prediction?
