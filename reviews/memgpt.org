#+TAGS: stoica llm mloptim

* MemGPT: Towards LLMs as Operating Systems
** Info
*Arxiv:* https://arxiv.org/pdf/2310.08560
*Authors:* Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, Joseph E. Gonzalez
*Date:* Feb 2024
** Review
*** Problem Being Solved?
LLMs have a strict context length. This is context length is gradually filled through the transformer architecture, resulting in a quadratic scaling of computation time with context lengths increase. Additionally, /Liu et al., 2023a/ shows uneven attention distribution over large context lengths. The problem is that LLMs can not access any data outside of their context lengths, restricting the effectiveness over large corpuses/long message histories.
*** What is an overview of the method?
The method comprises of splitting the context length given to the model and injecting information from archival storage. There are three components to the context length
 - System Instructions: A prompt to help the llm understand the system
 - Working Context: An area for the llm to do processing in and transfer it's understanding
 - FIFO Queue: A history of messages/responses from the archival storage to give to the model
[[./images/memgpt1.png]]
*** What are the metrics of success?
- Deep Memory Retrieval Task: Utilize a LLM judge to understand if MemGPT can maintain history of a conversation
- Conversation Opener Task: Does the LLM use prior knowledge to create engaging messages
- Multi-Document Question-Answering: Given a large corpus of documents (Wikipedia) can it draw the right document and provide information
- Nested Key-Value Retrieval: Additionally testing function chaining, can the LLM continue to recursively find the right Value if a prompt of Key-Key-Key-Value is given
*** What are the key innovations over prior work?
The inclusion of a queryable archival storage system through the use of "syscalls" given to the llm. This along with a queue manager, a function executor and working context allow for the LLM to reach back into a external storage system and retrieve information about past messages or a corpus of documents.
*** What are the key results?
On all the metrics listed aboe the MemGPT system performs quite well. They also did not provide any scaling limits and seem to have tested on the entire wikipedia corpus which is quite a large database.
*** How might this work have long term impact?
Context lengths in model, and the race to create bigger and bigger context lengths may end. Instead, we may see the use of archival storage in all premiere LLMs that allow them to maintain history over a very large duration of time.
** Questions
1. Can you do the same with a search engine backend? Yes, /Nakano et. al 2021/ has done something similar
2. How far can the "syscall" idea go? Seems like an interface into an external system is being provided here through the use of "syscalls". Can you provide additional "syscalls" to the system and use the same queuing/function executing method?
3. Why aren't the sizes of the context partitions adjustable by the LLM (e.g. increase "Working Context" or "FIFI Queue")?
4. This actually looks a bit like what chatgpt implemented recently to remember history through all your conversations
5. What safeguards does the system have to make sure that the function is called correct? There is a function validation system in place that makes sure it is called correctly. However, there may be an interested use case for automated reasoning here.
6. Just doing a basic vector search with pg does not seem like enough. It would be interesting if there was an implementation that makes the searching more efficient
7. How is the LLM Judge verified to be providing accurate responses?
8. Less a question, but more shocked how little actual implementation there is. The major implementation pieces seem to be:
   - Embeddings Search (but maybe actually not that much implementation here)
   - Function verifier
   - Queue Manager
