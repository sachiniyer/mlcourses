#+TAGS: CIL
#+OPTIONS: ^:{}

* A Theoretical Study on Solving Continual Learning
** Review
*** Info
- *Arxiv:* https://arxiv.org/abs/2211.02633
- *Authors:* Gyuhak Kim, Changnan Xiao, Tatsuya Konishi, Zixuan Ke, Bing Liu
- *Date:* Nov 2022
*** Problem Being Solved?
The paper aims to prove that OOD is important to the Continual Learning Problem. There are two types of Continual Learning, Class Incremental Learning (CIL) and Task Incremental Learning (TIL). TIL is essentially the same thing as Within-Task Prediction, so it is not considered strongly in the paper. Instead the paper focuses on a way to improve CIL.

The paper shows that a good Within-Task Prediction (WP) and Task-ID Prediction (TP) is necessary and sufficient for a good CIL. Additionally, Out of Distribution detection (OOD) + TP is necessary and sufficient for a good WP.
#+begin_src mermaid :file images/continual-learning-theoreticalp1.png
stateDiagram-v2
   CL: Continual Learning (CL)
   CIL: Class Incremental Learning (CIL)
   TIL: Task Incremental Learning (TIL)
   WP: Within-Task Prediction (WP)
   TP: Task-Id Prediction (TP)
   OOD: Out of Distribution Detection (OOD)
   CL --> CIL

   CL --> TIL
   CIL --> WP
   TIL --> WP

   CIL --> TP
   WP --> TP
   WP --> OOD
#+end_src

#+RESULTS:
[[file:images/continual-learning-theoreticalp1.png]]

*** What is an overview of the method?
The paper has two parts, a theoretical foundation for why this structure exists as it does. An experimental portion where CIL algorithms are paired with good OOD detection algorithms and the performance is show to increase. I'll skip the theoretical foundations (to hard to synthesize into something that can be explained easily).
*** What are the metrics of success?
- Average Classification Accuracy: How accurate is the method over all the classes after learning the last class
- Average AUC (Area Under the ROC Curve): [[https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/][ROC Curve Doc]]. Basically for a given task, measuring the AUC (binary being In Distribution and Out of Distribution) and then taking the average of that number across all of the tasks.
*** What are the key innovations over prior work?
1. Providing a solid theoretical foundation for why OOD detection matters
2. Backing up that theoretical foundation with emperical results that also prove that OOD detection matters
*** What are the key results?
They show that a good OOD detection algorithm (e.g. ODIN or CSI) can significantly improve the accuracy of a CIL model (e.g. HAT or Sup). They show that the addition of CSI to HAT or Sup outperform the baselines by a large margin.
*** How might this work have long term impact?
- Implication for regularization and replay methods: It is important to learn the OOD detection and minimize for catastrophic forgetting together.
- Implication for open-world learning: New tasks can continually be discovered through OOD detection and learned, potentially allowing for self-supervised learning.
- WP and TP/OOD need to be optimized jointly to ensure that OOD does not affect previous tasks
** Questions
- 2 Related Work
  - What is a PAC-Bayesian framework?
  - What is orthogonal gradient descent?
- 3.3 Task Prediction (TP) to OOD Detection
  - Didn't understand Bounding TP cross entropy by OOD cross entropy well, but get the general idea

* Detailed Review
** O Abstract
There are two types of Continual Learning, Class Incremental Learning (CIL) and Task Incremental Learning (TIL). The main problem with both is catastrophic forgetting (CF).
- TIL: Catastrophic forgetting is mostly solved
- CIL: Catastrophic forgetting is still an issue due to inter-task class separation
  - Within-Task Prediction (WIP): Effectively the same as TIL
  - Task-Id Prediction (TIP): This is main problem, and is correlated with the Out-Of-Distribution (OOD) technique

This paper also shows that WIP and TIP are /necessary and sufficient/ for good CIL performance.
** 1 Introduction
CIL: The learning process builds a single classifier for all the tasks/classes learned so far. Any test instance from any class can be presented for classification. No task information (e.g. task id) will be presented at test time.
*** Class Incremental Learning
$D_k = \{(x_k^i, y_k^i)\}_{i=1}^{n_k}$ describes the training dataset for each task $k$ where $x_k^i$ is an input sample and $y_k^i$ is a label for that sample.

The goal is to create a model such that given any $x$ sample you can get to a $y$ label. In training, the task ids may be presented, but in testing, they may not be presented
*** Task Incremental Learning
$D_k = \{((x_k^i, k), y_k^i)\}_{i=1}^{n_k}$ the big difference here from CIL is $x_k^i$ vs $(x_k^i, k)$. The above definitions work the same here.

The goal is to create a model such that for any $(x, k)$ you can find a corresponding $y$.
*** Open world setting issues with CIL
The main concept being analyzed here is how In-Distribution (IND) samples play with Out-Of-Distribution (OOD) samples (in testing). The main issue in open world settings (where tasks have not always been seen before) is to figure out IND/OOD. This paper will do the following:
1. Create a theoretical foundation for:
   - defining how CIL performance is bounded by WP and TP performances
   - TP and task OOD detection performance bound each other
2. Design a better CIL algorithm based of those theoretical foundations
** 2 Related Work
There is not much work to create theoretical guidance on how to solve the CIL problem (the paper states this )

Existing approaches:
- /regularization/: minimize changes to model parameters learned from previous tasks
- /replay/: Memorize some old examples and use them to jointly train the new tasks
- /pseudo-replay/: Synthesize data on the old task and use that to train the new tasks
- /Parameter Isolation/: Create subnets from the overall network to learn different tasks (e.g. HAT)

This work is different than previous works, because instead of focusing on traditional generalization bounds, this work focuses specifically on optimizing the CIL problem.
** 3 CIL by Within-Task Prediction and Task-ID Prediction
Two goals:
1. Show that CIL performance improves if the within-task prediction (WP) improves and/or the task-id prediction (TP) improves.
2. Show that TP and OOD detection bound each other which shows CIL performance is defined by WP and OOD
3. Define the necessary conditions for a good CIL model (good WP and good TP/OOD)

*** 3.1 CIL Problem Decomposition
**** Definition of the input/tasks
Tasks are defined as $\{(X_k, Y_k)\}_{k=1,\ldots,T}$. $X_k$ is the domain of task $k$. $Y_k$ are the classes of task $k$. $Y_k$ can be further decomposed into $Y_{k,j}$s where $j$ is the class of task $k$. In other terms, $k$ is the superclass, while $j$ is the class of an input.
***** Assumption 1
The classes of the same task are disjoint (there is no overlap between classes of the same task)
***** Assumption 2
The domains of tasks are disjoint (there is no overlap between the tasks themselves)
**** Decomposition into WP and TP
Goal of CIL problem: $P(x \in X_{k,j} | D)$. The probability of the $k,j$ superclass class pairing for a given input $D$

Expanded out the WP and TP probabilities are:
- WP: $P(x \in X_{k,j} | x \in X_k, D)$. Given the superclass $k$, what is the probability the input $D$ corresponds to class $j$
- TP: $P(x \in X_k | D)$. What is the probability input $D$ corresponds to class $k$

Based off those two assumptions, we can expand out $P(x \in X_{k,j} | D)$ to the following

\begin{equation}
P(x \in X_{k_0,j_0} | D) = \sum_{k=1,\ldots,n}{P(x \in X_{k,j_0} | x \in X_k, D)P(x \in X_k |D)} \\
= P(x \in X_{k_0,j_0} | x \in X_{k_0}, D)P(x \in X_{k_0} | D)
\end{equation}
- Part 1: The probability input D is the superclass, class pairing $k_0$, $j_0$
- Part 2: Definition for each of the different $k$ superclasses (representing the sum over each of the superclasses) and their probability leading into $j_0$
- Part 3: Scoped back down for $k_0$ because of the Probability definition from the beginning.

$P(x \in X_{k_0,j_0} | x \in X_{k_0}, D)$ here defines the WP probability. What are the chances, given superclass $k_0$, D corresponds to class $j_0$

$P(x \in X_{k_0} | D)$ here defines the TP probability. What are the chances, input D corresponds to superclass $k_0$
**** Remarks
- $P(x \in X_{k_0,j_0} | x \in X_{k_0}, D)P(x \in X_{k_0} | D)$ shows that improving either of the Probabilities (WP or TP) will improve CIL performance
- This has nothing do with the training process or learning algorithms, but takes the probabilities as givens
- Referring back to the assumptions, there does not say anything about classes crossing task/superclass boundaries (blurry tasks). It merely assumes that the tasks themselves are disjoint.
- $\textbf{CIL} = \textbf{WP} * \textbf{TP}$, which implies that CIL problems can be described by their probabilistic definitions for WP and TP
*** 3.2 CIL Improves as WP and/or TP Improve
This study analyzes performance with a /trained CIL model/, and uses *cross-entropy* as the performance measure of the trained model.

Cross-Entropy definition over two probability distributions $p$ and $q$: $H(p, q) := -\mathbb{E}_p[\log q] = - \sum_{i} p_i \log q_i$ (this is just the basic cross entropy equation)

For a $x \in X$, let $y$ be the ground truth label. $y_{k_0,j_0} = 1$ if $x \in X_{k_0,j_0}$ and $y_{k_0,j_0} = 0$ if $(k,j) \ne (k_0,j_0)$ (specifically $\forall (k,j) \ne (k_0,j_0)$). $\tilde{y}$ is the WP ground truth label and $\bar{y}$ is the TP ground truth label. $\tilde{y}$ and $\bar{y}$ follow the same pattern as $y$ when setting $1$ and $0$.

Based off this notation, we can set the following definitions for cross-entropy for CIL, WP and TP:
- WP: $H_{WP}(x) = H(\tilde{y}, \{P(x \in X_{k_0,j} | x \in X_{k_0}, D)\}_j)$
- TP: $H_{TP}(x) = H(\bar{y}, \{P(x \in X_{k}|D\}_{k}))$
- CIL: $H_{CIL}(x) = H(y,\{P(x \in X_{k,j} | D\}_{k,j})$

**** Theorem 1
when $H_{TP}(x) \le \delta$ and $H_{WP}(x) \le \epsilon$ then $H_{CIL} \le \delta + \epsilon$. This definition says that the two cross entropies (TP and WP) are what compose the CIL cross entropy. This makes the /necessary and sufficient/ clause from the Introduction.

More detailed proof of this is given in [[https://arxiv.org/pdf/2211.02633#theorem.1][A.1]] (where $H_{CIL}$ is expanded out and decomposed into $H_{WP}$ and $H_{TP}$).
**** Corollary 1
Corollary 1 expands on Theorem 1 to relate CIL to TP and WP individually (if TP is held constant, CIL is dependent on WP performance and vice versa).

These two statements are formally defined by the following ($U(X)$ refers to the uniform distribution):
- Take $\( \mathbb{E}_{x \sim U(X)}[H_{TP}(x)] \leq \delta \)$. Then $\mathbb{E}_{x \sim U(X)}[H_{CIL}(x)] \leq \mathbb{E}_{x \sim U(X)}[H_{WP}(x)] + \delta$ (CIL is dependent on WP performance - TP is held constant)
- Take $\( \mathbb{E}_{x \sim U(X)}[H_{WP}(x)] \leq \epsilon \)$. Then $\mathbb{E}_{x \sim U(X)}[H_{CIL}(x)] \leq \mathbb{E}_{x \sim U(X)}[H_{TP}(x)] + \epsilon$ (CIL is dependent on TP performance - WP is held constant)

*** 3.3 Task Prediction (TP) to OOD Detection
**** Definition of OOD
Again, use cross-entropy $H$ to measure performance of TP and OOD detection.
- Probability he $k$th task says input $D$ is OOD: $P_{k}'(x \in X_k | D)$

$P_{k}'(x \in X_k | D)$ (OOD probability) is a Bernoulli distribution while $P(x \in X_k | D)$ (TP probability) is a categorical distribution over the superclasses/tasks

OOD Detectors can be defined with two methods:
1. Using the output values corresponding to the classes of the task
   - By taking sigmoid of maximum logit value
   - Maximum Softmax Probability after re-scaling to 0 to 1 (rescaling the logits)
2. Defining the OOD detector as a function of tasks
   - Mahalanobis distance: distance between a point and a distribution $D_M(\mathbf{x}, \boldsymbol{\mu}) = \sqrt{(\mathbf{x} - \boldsymbol{\mu})^\top \mathbf{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})}$
     - $x$: The data point whose distance is being measured
     - $\mu$: The mean vector of the distribution
     - $\Sigma$: The covariance matrix of the distribution - the covariance between each variable in the matrix [[https://www.youtube.com/watch?v=152tSYtiQbw][more on this]]

The cross entropy loss for OOD detector is defined by:
\[
H_{\text{OOD},k}(x) = \begin{cases}
H(1, \mathbf{P}'_k(x \in \mathbf{X}_k | D)) = -\log \mathbf{P}'_k(x \in \mathbf{X}_k | D), & \text{if } x \in \mathbf{X}_k, \\
H(0, \mathbf{P}'_k(x \in \mathbf{X}_k | D)) = -\log \mathbf{P}'_k(x \notin \mathbf{X}_k | D), & \text{if } x \notin \mathbf{X}_k.
\end{cases}
\]

**** Theorem 2
***** Bounding OOD cross entropy by a fixed TP cross entropy
Assume $x \in \textbf{X}_{k_0}$
- Case $k=k_0$: $H_{OOD,k_0}(x) = -\log \mathbf{P}'_{k_0}(x \in \mathbf{X}_{k_0} | D) = -\log \mathbf{P}(x \in \mathbf{X}_{k_0} | D) = H_{TP}(x) \le \delta$
- Case $k \ne k_0$: Refer to [[https://arxiv.org/pdf/2211.02633#subsection.A.3][appendix]], the tricky part is $-\log(\mathbf{P}(x \in \cup_{k' \ne k}X_{k'}|D)) \le -\log(\mathbf{P}(x \in X_{k_0} | D)$. $\cup_{k' \ne k}$ defines the set of all tasks that is not equal to $k'$. The probability of that set must be $\ge$ than the prob of $k_0$ (which is then $\le$ because of the $-\log$). the $\ge$ inequality exists because $k_0$ is a subset of that $\cup_{k' \ne k}$ set union.
***** Bounding TP cross entropy by OOD cross entropy
Assume $x \in \textbf{X}_{k_0}$
- Case $k=k_0$: $H_{OOD}(x) \le \delta  $ by the definition of cross entropy means $-\log{\mathbf{P}'_{k_0}{x \in \mathbf{X}_{k_0} | D) \le \delta_{k_0}$ which leads to $\mathbf{P}'_{k_0}(x \in \mathbf{X}_{k0}|D) \ge e^{-\delta_{k_0}}$
- Case $k \ne k_0$: look at [[https://arxiv.org/pdf/2211.02633#subsection.A.3][appendix]]. TODO: understand this better
**** Theorem 3
Basically apply the analysis for WP and TP and the relation into CIL in 3.2 and plug in OOD instead (actually nothing more than this)
*** 3.4 Necessary Conditions for Improving CIL
This section is just done for completeness. Basically combining all of the previous theorems and doing the following:
- Cross Entropy of $H_{WP}(x) = H_{CIL}(x) \le \eta$ , $H_{TP}(x) \le H_{CIL}(x) \le \eta$, and $H_{OOD,i}(x) \le H_{TP}(x) \le \eta$ which sets everything below $\eta$
Intuitively, this makes sense given the relations in 3.2
** 4 New CIL Techniques and Experiments
Goal is to slap on some existing OOD algorithms onto pre-existing CIL models and see the performance increase.
*** 4.1 Datasets, CL Baselines and OOD Detection Methods
**** Datasets
1. MNIST (CIL task with 5 tasks with 2 consecutive digits as a task)
2. CIFAR-10 (5 tasks with 2 consecutive classes as a task)
3. CIFAR-100 (10 tasks, 10 classes) and (20 tasks, 5 classes)
4. Tiny-ImageNet (5 tasks, 40 classes) and (10 tasks, 20 classes)
**** Baseline CL Methods
- regularization: MUC and PASS
- replay: LwF, iCaRL, Mnemonics, BiC, DER++, and CO^{2}L
- orthogonal projection: OWM
- parameter isolation: CCG, HyperNet, HAT, SupSup (Sup), PR
**** OOD Detection Methods
- ODIN: representative method
- CSI: based on supervised contrastive learning. "rotation data augmentations create distributional shifted samples to act as negative data" More in Appendix D
*** 4.2 Training Details and Evaluation Metrics
**** Training Details
Backbone:
- AlexNet: MNIST
- ResNet-18: CIFAR-10, CIFAR-100, Tiny-ImageNet

OWN and HyperNet have their own architectures (but this did not perform well anyway)

For replay methods, memory buffer size is:
- MNIST: 200
- CIFAR-10: 200
- CIFAR-100: 2000
- Tiny-ImageNet: 2000

Hyperparameters suggested by the authors is what is used. If the results could not be replicated, a grid search is done over the hyperparameters with 10% of the training set.
**** Evaluation Metrics
1. /Average classification accuracy/: what was the classification accuracy of the models (also report /forgetting rate/ in Appendix J)
2. /Average AUC/: Average Area Under the ROC Curve (Probability that the model will rank a randomly chosen positive sample higher than a randomly chosen negative sample [[https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc][more info]]). Average $AUC$ means $AUC = \sum_k \frac{AUC_k}{n}$ where $n$ is the number of tasks.
**** Prediction Methods
Use $P(x \in X_{k_0,j_0} | x \in X_{k_0}, D)P(x \in X_{k_0} | D)$ ($P(WP)P(TP)$) to find Prediction
- WP: Just take the softmax values of the classes in each task
- TP: Do one of the following:
  - For single classification head use $\hat{y} = \textbf{argmax}\{f(x)\}$ where $f(x)$ is the logit output of the network
  - For multi-head methods (HAT, HyperNet, and Sup) use a concatenated output $\hat{y} = \textbf{argmax}\{\bigoplus_{k}f(x)_k\}$
    - All of the logits are taken together and smashed into one single vector and then argmax is taken.
*** 4.3 Better OOD Detection Produces Better CIL Performance
**** Applying ODIN
Steps:
- Train the baseline models using their original algorithms
- Apply temperature scaling and input noise of ODIN at testing for each task (ODIN specific thing)
Results:
[[./images/continual-learning-theoreticalp2.png]]
Overall, there are performance increases in Classification across the board, while the AUC does not always improve.
**** Applying CSI
Could not apply CSI to any of the backbones except for HAT and Sup (which is analyzed in the next section).
*** 4.4 Full Comparison of HAT+CSI and Sup+CSI with Baselines

**** Why these two combinations are selected:
- HAT and Sup are TIL system and have very little or no CF
- CSI is a stronger OOD method than ODIN
- HAT and Sup don't need to save any previous task data
CSI is better that ODIN:
[[./images/continual-learning-theoreticalp3.png]]
**** Results
A calibration method is used (denoted by $+c$ in the table) that optimizes HAT and Sup by including a small replay buffer of tasks. The accuracy is much higher than all the other baselines
[[./images/continual-learning-theoreticalp4.png]]
Additionally, the TIL component improved with the methods given.
*** 4.5 Implications for Existing CL Methods, Open-World Learning and Future Research
**** Implication for regularization and replay methods
/inter-task class separation/ is low in regularization-based methods because they do not consider OOD detection. Replay methods are better (other tasks are naturally OOD for the current task), however, the replay data is small and the resulting OOD is suboptimal.
**** Implication for open-world learning
OOD detection lends itself well to bringing open-world learning to CL. This is because autonomous agents can learn inputs as OOD automatically which would lead to "curiosity-driven self-supervised learning".
**** Limitation and future work
There are future optimizations to be made in how the input data is represented. There is no guidance on how to get good CIL results in this paper. If "/holistic feature representations/" of the input data are made, then IND detection and OOD detection would become more accurate
** Conclusion
This paper gives theoretical and experimental proof that OOD detection is necessary for a performant CIL system.
** Appendix C Definitions of TP
TODO
** Appendix D Details of HAT, Sup, and CSI
TODO
