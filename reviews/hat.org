#+TAGS: CIL


* Overcoming Catastrophic Forgetting with Hard Attention to the Task
** Review
*** Info
*Arxiv:* https://arxiv.org/pdf/1801.01423
*Authors:* Joan Serra Dıdac Surıs Marius Miron Alexandros Karatzoglou
*Date:* May 2018
*** Problem Being Solved?
Catastrophic forgetting is the tendency for neural networks when performing continual learning to forget older tasks. Essentially, the more epochs away from the task learned, the harder it is to "remember" how to perform that task.
*** What is an overview of the method?
The main way catastrophic forgetting is avoided is by accumulating an "almost binary attention vector" while learning each of the tasks. This vector is controlled by two hyperparameters, the width of the vector and the magnitude of the annealing constant (for the activation function of the vector e.g. sigmoid). The effect of this method is that each task bases their vector on the previous tasks but adds the neurons that they find most important.
*** What are the metrics of success?
The goal is twofold; to provide good accuracy on each of the tasks and minimize the "forgetting ratio" (comparison between the accuracy of different tasks).
*** What are the key innovations over prior work?
The innovations are that this learned vector provides a much smaller forgetting ratio over previous works, while maintaining accuracy of the tasks. They also show that performance is maintained up to 10 tasks, and mention that performance is maintained further.
*** What are the key results?
They consider many datasets (CIFAR10/100, FaceScrub, FashionMNIST, NotMNIST, MNIST, SVHN, TrafficSigns), and find a higher performance of HAT than other methods (PNN, PathNet, EWC, IMM...). Additionally, they show that there is not a super high sensitivity to hyperparameters and that performance and default to $s_{max} = 400$ and $c = 0.75$.
*** How might this work have long term impact?
This work shows a large reduction in Catastrophic Forgetting and has become the standard method to perform CIL on tasks.
** Questions
1. Still missing understanding of how the gradient calculations are performed, but have intuition about it.
