#+TAGS: CIL


* HAT-CL: A Hard-Attention-To-The-Task PyTorch Library For Continual Learning
** Review
*** Info
*Arxiv:* https://arxiv.org/pdf/2307.09653
*Authors:* Xiaotian Duan
*Date:* Feb 2024
*** Problem Being Solved?
Implementations of HAT are hairy because of the manual gradient manipulation that happens in the backward step. Additionally, applying the masks to existing networks can be tedious and therefore prone to error (especially for transformer networks like ViT). A better method for implementation is needed.
*** What is an overview of the method?
- Algo changes
  - Initialization Scheme: Instead of Gaussian, they init everything (and follow a cosine curve for decay?)
  - Incorporates a task quota for capacity (do not penalize if a task does not fully utilize capacity)
  - Regularization Per Layer (instead of a regularization constant over all norm functions)
- Implementation changes
  - Create a ~HATPayload~ class that encapsulates all the important HAT information (task id, mask scale, previous masks...)
  - ~HATPayload~ has a lazy mask application which it easier to keep the original data until the mask is absolutely necessary
  - ~HATPayload~ has most of the common tensor operations allowing it to be embed into other networks.
*** What are the metrics of success?
- Ease of Implementation
- Consistent or higher performance
*** What are the key innovations over prior work?
Mostly explained in the overview, but the key innovation is a restructuring of the traditional implementation
*** What are the key results?
The key resulst
*** How might this work have long term impact?
This will make it much easier to add the hat technique to different models when doing OOD benchmarking or other CIL tasks.
** Questions
- Would like to see more benchmarking data
- Seems actually super easy to use in practice
