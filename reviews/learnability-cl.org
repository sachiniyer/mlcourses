#+TAGS: CIL

* Learnability and Algorithm for Continual Learning
** Review
*** Info
- *Arxiv:* https://arxiv.org/abs/2306.12646
- *Authors:* Gyuhak Kim, Changnan Xiao, Tatsuya Konishi, Bing Liu
- *Date:* Jun 2023
*** Problem Being Solved?
We want to prove the learnability of CIL (will all the components of a CIL algorithm converge given the correct dataset)
*** What is an overview of the method?
The proposed method ROW has the following features:
- Replay based method
- at each task $k$, the system receives dataset $D_k$ and uses the replay data (in memory buffer $M$) as OOD data to train the OOD detection model (and also fine-tune the WP head)
- The model has two heads: one OOD head and one WP head (OOD head is $h_k$ with parameter set $\theta_k$, WP head is $g_k$ with parameter set $\phi_k$) (there are these two heads for every task that we have)
- Optimization happens on the three parameter sets $(\psi_k, \theta_k, \phi_k)$, where $\psi_k$ is the feature extractor $f_k$'s parameter set
*** What are the metrics of success?
- Average Classifcation Accuracy across the tasks is high
- Average Forgetting Rate across the tasks is low
*** What are the key innovations over prior work?
The addition of an OOD detection algorithm to the existing TIL algorithm
*** What are the key results?
The ROW algorithm performs better than most of the other baselines
*** How might this work have long term impact?
This shows a new method for CIL algorithms that is based in OOD detection
** Questions
- Is the definition of a training set correct in *Definition 1.2* (should what they have as the training set actually be the testing set)
- Don't understand Orthogonal projection methods for TIL methods (PCP, CUBER)
- Don't understand the implementation differences between *i* and *ii* in Introduction (seems like different wordings of the same thing)
- There is some intuition about the Risk function that I am missing
- need to read through the proofs in the appendix
- want to understand mahalanobis distance again
  - Why is this different than what we are doing? - they are taking the average of the feature set across all of the samples. I think that it is different because they are just using the distance of the input samples feature set against the classes as a constant, while our method would use the distance number as part of the loss generation (making the OOD more efficient as it would consider /inter-task separation/).
* Detailed Review
** 1 Introduction
Continual Learning is important and there is a key challenge of /catastrophic forgetting/ (when learning new tasks, the system forgets old tasks).

There are two continual learning settings
- TIL: /task incremental learning/
- CIL: /class incremental learning/

*** TIL/CIL/OOD Definitions
**** TIL definition
Learning a sequence of tasks $1,2,...,T$ with a training set defined for task $k$ as $D_k = \{((x_k^i, k), y_k^i)_{i=1}^{n_k}\}$ where $n_k$ is the number of samples in the task. $y_k^i$ is the given class for a superclass $Y_k$ which is part of the set of superclasses $\Upsilon$. The system learns to assign a class label to an input sample given the superclass it is classifying for.
**** CIL definition
*Key Difference from TIL*: At test time, no task-id is provided for a test instance

Training set is $D_k = \{(x_k^i, y_k^i)_{i=1}^{n_k}\}$. The only qualification that is different from TIL is that all $Y_k$ are disjoint (the superclasses don't overlap, however the classes may overlap). The challenge comes at inference time where the goal become to separate out each of the superclasses (defined as /inter-task class separation/ (ICS)). In addition to /catastrophic forgetting/ for previous classes, the system must maintain /inter-task class separation/ when defining the OOD/IND mechanism.
**** OOD definition
given the training set $D = \{(x^i, y^i)_{i=1}^{n}\}$, learn a function so that the samples can be classified as IND (in distribution) or OOD (out of distribution). More precisely $f : X \rightarrow \Upsilon \cup \{O\}$ (union of the set of classes and the OOD class (WP prediction can happen here)). Further *if every task's OOD is perfect, we know which task the sample belongs to (because all other tasks will say it is OOD)*.
*** Proving Learnability of CIL
The goal of this paper is to perform the *CIL learnability* study. Which requires two assumptions.
- OOD detection is learnable
- There is a mechanism that can overcome /catastrophic forgetting/ (HAT/SUP do this through parameter isolation, PCP/CUBER through orthogonal projection methods).

To solve CIL, there needs to be a combination of the following:
- *a*: a TIL method that can protect each task from /catastrophic forgetting/
- *b*: a supervised method for learning the tasks themselves
- *c*: an OOD detection method

*b* and *c* can be combined using either of the following methods:
- *i*: OOD method also does the supervised learning of the tasks (it also learns the IND classes)
- *ii*: a WP model that can also perform OOD detection

*** Goals of the paper
1. Perform the first learnability study of CIL (theoretical analysis)
2. Create a new empirical method for CIL (they call this ROW - Replay, OOD, and WP for CIL)

The theory is also applicable to open world learning (you need OOD and CIL for open world classification problems)
** 2 Related Work
*** Theoretical Side
- PAC-Bayesian framework provides bound on expected error by average loss on observed tasks (but this is only about TIL, and creates a separate model for each task).
- There is some research on task similarity, but again this is only for TIL
- Orthogonal gradient descent (OGD) gives a tighter generalization bound than SGD for these TIL problems

This work is pretty different because we are looking mostly at OOD and CIL (and are not necessarily concerned exactly with the prediction of the tasks themselves).
*** Empirical Side
- Regularization-based methods: restric the learned parameters from old tasks from being updated in new tasks using regularization methods
- Replay-based methods: save a small amount of training data from old tasks and jointly train the model using the saved data and the current data (there is also analysis on which sample specifically should be used for replay)
- Pseudo-replay methods: autogenerate some replay data to simulate the data from the old tasks
- Parameter-isolation methods: train and protect a sub-network for each task (HAT and SupSup are in this category). This has been great at eliminating catastrophic forgetting
- Orthogonal projection: learn each task in an orthogonal space to reduce task interference or CF (does this mean doing something similar to the hyperspherical embeddings)

This paper uses the following techniques:
- use the replay data as OOD training data to fine-tune an OOD detection head for each task based on the features learned for the WP head
- use the HAT method to overcome CF

There are some existing OOD methods, but most are either unrealistic or perform poorly. They also don't deal with the /inter-task class separation/ (ICS) issue.
** 3 Learnability of CIL
CIL prediction probability is a combination of the WP and TP prediction probabilities $\mathbf{P}(X_{k,j}|x) = \mathbf{P}(X_{k,j}|x,k)\mathbf{P}(X_k|x)$. $\mathbf{P}(X_{k,j}|x,k)$ is the WP probability and $\mathbf{P}(X_k|x)$ is the TP probability. However, these don't have an implication on the learnability of the CIL algorithm itself.

Given the TIL algorithms that exist (HAT, SupSup), the paper assumes that all tasks are learned without /catastrophic forgetting/. Additionally, recent works (they site Fang et al.) show that OOD detection is learnable.

*** Definitions of the pieces
Definitions:
- $X$: a feature space
- $Y$: a label space
- $H$: a hypothesis function space. $H$ is a ring
- $k$: a certain task
- $D_{(X_k,Y_k)}$: distribution of the input space for task $k$
- $l(y_1,y_2) \ge 0$: loss function
- $h \in H$: is a singular hypothesis function
- $\mathbf{R}_{D_{(X, Y)}}(h) \stackrel{\text{def}}{=} \mathbb{E}_{(x, y) \sim D(X, Y)} \big[l(h(x), y)\big]$: Risk function for any given $X$ and $Y$
- $S \stackrel{\text{def}}{=} \{(x, y) \sim D(X, Y)\}$: sample from $D_{(X,Y)}$ notated as $S \sim D_{(X,Y)}$
- $D_{X_1,Y_1},\ldots,D_{(X_{\tau},Y_{\tau})}$: a series of distributions
- $D_{[1:k]} = \frac{\sum_{i=1}^{k}\pi_i D_{(X_i,Y_i)}}{\sum_{i=1}^k \pi_i}$: denotes a mixture of the first k distributions (where the $\pi_i$s represent a probability distribution (sum to $1$ and are all $>0$).
- $S|_{[k1:k2]}$ follows similarly from $D_{[k1:k2]}$ (a sampling of each of the $s$ in the distribution defined by $supp$).
- $h_k(x) = \mathbf{A}_k(S)(x)$: hypothesis function for task $k$ (this is found after training the $k$th task) (this is also only well-defined for the tasks trained up till this point)
- $h_k = \text{argmax}_{1 \le i \le k, j \in \{1,\ldots\}}\{\ldots,z_k^{i,j},\ldots\}$: hypothesis function (basically argmax over each $z_k^{i,j}$ which represents the logits (or score function?))
- $[l(\text{argmax}_{1 \le i \le k, j \in \{1,\ldots\}}\{\ldots,z_k^{i,j},\ldots\}, y)]$: loss numbers where $l$ is the loss function and $y$ is the label
- $h_k = \text{argmax}_{1 \le i \le k, j \in \{1,\ldots\}}\{\ldots,z_k^{i,j},\ldots;z_k^o\}$: hypothesis score with an additional OOD class
**** What is a ring
[[https://planetmath.org/ringofcontinuousfunctions][more info]]
Classifying a function space (in this case the hypothesis function space $H$), means that the functions in the space all satisfy some basic properties.

Namely the ones the paper cares about are (more properties in the link above):
- addition: $(f+g)(x) := f(x) + g(x)$
- multiplication: $(fg)(x) := f(x)g(x)$
**** What is a risk function
[[https://mlweb.loria.fr/book/en/risk.html][more info]]
A risk function is used to evaluate the performance of estimators and decision rules. It is the expected loss or error of the hypothesis function when applied to new data.
**** What does $supp$ mean
$supp$ means the support of the distribution. This is the set of all elements that have a non-zero probability in the distribution.
**** What is a hypothesis function
Gut check on this one, just the predictive function that is the result of the model
*** Definition 3.1
For fully observable distributions $D$. And a closed world (no OOD)
distribution $D$, hypothesis function space $H$ can be applied to algorithm $\mathbf{A}$ and a sequence of $\{\epsilon_n|\lim_{n\rightarrow \infty}\epsilon_n = 0\}$ (error rates decreasing towards zero) such that
1. The tasks's individual data distributions are all disjoint (don't share samples)
2. All the probabilities are greater than 0 and sum to 1

And then the risk function's expected values is less than the constant error rate ($\epsilon_n$). Additionally, the risk function is calculated over the distribution's $D_{[1:k]}$ which means that the current task and all the previous task's data are visible. Because we don't have access to all of the data at the same time, $D$ will be partially observable. Therefore, we define the generic algorithm recursively as $A_k^\tau(S) = A_k^\tau(S|_k,\mathbf(A)_{k-1}^\tau(S|_{[1:k-1]})$. This also depends on the underlying TIL algorithm being able to handle catastrophic forgetting well.
*** Definition 3.2
For only partially observable distributions $D$. And a closed world (no OOD)

The key difference is that the past tasks are not visible. This basically just becomes a TIL problem which is solved (shown through Theorem 3.3)
*** Definition 3.4
For fully observable distribution $D$, and an open world (OOD)

The same two qualification from 3.1 are there, but with the addition of
3. for any $O_{(X_1, Y_1)},\ldots,O_{(X_T,Y_T)} \in D$ any $\alpha_1,\ldots,\alpha_T \in [0, 1)$ (the ood dataset is just comprised of 0s and 1s - binary)

Definition 3.4 being satisfied means that Definition 3.1 is also satisfied - Theorem 3.5
*** Definition 3.6
For only paritially observable distributions $D$, and an open world (OOD)

Very similar dataset structure to 3.4, except we only see one task at a time. The paper cites Fang et al. as showing that OOD detection is learnable and therefore they derive that CIL is learnable as OOD detection is learnable (by converting CIL learning to a series of OOD learning problems) TODO: go through the proofs in the appendix

3.6 is enough to prove 3.4 as given by Theorem 7 (proof in appendix)
** Proposed method
OOD is also capable of classification. The masks in HAT are used to protect each OOD model to ensure there is no forgetting. It is also possible to introduce a WP head so that OOD only has to estimate TP instead of WP and TP.

The proposed method ROW has the following features:
- Replay based method
- at each task $k$, the system receives dataset $D_k$ and uses the replay data (in memory buffer $M$) as OOD data to train the OOD detection model (and also fine-tune the WP head)
- The model has two heads: one OOD head and one WP head (OOD head is $h_k$ with parameter set $\theta_k$, WP head is $g_k$ with parameter set $\phi_k$) (there are these two heads for every task that we have)
- Optimization happens on the three parameter sets $(\psi_k, \theta_k, \phi_k)$, where $\psi_k$ is the feature extractor $f_k$'s parameter set

The training steps are:
1. Train the feature extractor $f_k$ and OOD head $h_k$ using both the IND instances in $D_k$ and the OOD instances in $M$ (replay data)
2. fine-tune a WP head $g_k$ for the task using $D_k$ based only on the fixed feature extractor $f_k$ (OOD has nothing to do here)
3. fine-tune the OOD heads of all the tasks that have been learned so far

The outputs of $h_k$ and $g_k$ are what comprises the final prediction

#+begin_src mermaid :file images/learnability-clp1.png
flowchart LR
   ind[In Distribution Sample]
   ood[Out of Distribution Sample]
   f[Feature Extractor]
   h[OOD Head]
   g["WP Head (class prediction)"]
   po["Out of Distribution Prediction"]
   pc["Class Prediction"]
   pf["Final Prediction"]

   ind --> f
   ood --> f
   f --> h
   f --> g
   h --> po
   g --> pc
   po --> pf
   pc --> pf
#+end_src

#+RESULTS:
[[file:images/learnability-clp1.png]]

*** Training the feature extractor and the OOD head
Training the OOD head $h_k$ for task k.

#+begin_src mermaid :file images/learnability-clp2.png
flowchart LR
   ind[In Distribution Sample]
   ood[Out of Distribution Sample]
   f[Feature Extractor]
   h[OOD Head]
   po["Out of Distribution Prediction"]

   ind --> f
   ood --> f
   f --> h
   h --> po
#+end_src

#+RESULTS:
[[file:images/learnability-clp2.png]]

The In Distribution samples are given by $D_k$, and the Out of distribution samples are given by $D_{k'} \in D$ where $k' \ne k$. The network of $h_k \circ f_k$ (combination of the feature extractor and the OOD head) is trained to max two things:
1. IND sample: the probability $p(y|x,k) = \text{softmax}h_k(f(x,k;\psi_k);\theta_k)_y$
2. OOD sample: $p(ood|x,k)$

The combination of these two losses is given as $L_{ood}(\psi_t, \theta_t) = -\frac{1}{2N}\left(\sum_{(x,y) \in D_K}\log{p(y|x,k)} + \sum_{(x,y) \in M}\log{p(ood|x,k)}\right)$. The replay instances are upsampled to achieve an equal number of samples as the current task data $D_k$.
*** Fine-Tuning the WP Head
Simple here. Just use cross-entropy loss
$L_{WP}(\phi_k) = -\frac{1}{N} \sum_{(x,y)\in D_k}\log{p(y|x,k)}$. The probabilities of the classes are just taking the softmax

#+begin_src mermaid :file images/learnability-clp3.png
flowchart LR
   ind[In Distribution Sample]
   f[Feature Extractor]
   g["WP Head (class prediction)"]
   pc["Class Prediction"]

   ind --> f
   f --> g
   g --> pc
#+end_src

#+RESULTS:
[[file:images/learnability-clp3.png]]

*** Fine-tuning the OOD Heads of All Tasks
The OOD heads that are trained first (because the $M$ memory is less diverse - less OOD samplings). To mitigate, train all the previous OOD heads after training each task using only the replay data in $M$ (take out that classes data, use the extracted classes as IND data, and the rest as OOD data). Use the same loss function as above (without the feature parameters) to train again.

#+begin_src mermaid :file images/learnability-clp4.png
flowchart LR
   ind[In Distribution Sample]
   ood[Out of Distribution Sample]
   f[Feature Extractor]
   h1[OOD Head 1]
   h2[OOD Head 2]
   h3[OOD Head 3]
   h4[OOD Head 4]
   po1["Out of Distribution Prediction1"]
   po2["Out of Distribution Prediction2"]
   po3["Out of Distribution Prediction3"]
   po4["Out of Distribution Prediction4"]

   ind --> f
   ood --> f
   f --> h1
   f --> h2
   f --> h3
   f --> h4
   h1 --> po1
   h2 --> po2
   h3 --> po3
   h4 --> po4
#+end_src

#+RESULTS:
[[file:images/learnability-clp4.png]]

*** Distance-based Coefficient
The paper notes that performance can be improved by using a "distance-based coefficient defined at the feature level into the output from the OOD head". This means that the OOD detection works better if there is a "distance-based coefficient" (something like mahalanobis score) defined at the level of the features themselves (not necessarily at the output of whether it is OOD or not). After training task $k$ , find the mean/variance of the feature vectors per class of the task and compute Mahalanobis distance $c_k(x) = \text{max}_y \frac{1}{MD(x; \mu_y, \sum_k)}$ where $\mu_y$ is the mean of the class $y$ and $\sum_k$ is the variance. The coefficient will be larger if the features of a test instance is closer to one of the sample means of the task, and smaller otherwise

TP Probability now is $\mathbf{P}(X_k|x) = c_k(x)\max_j \text{softmax}(h_k(x))_j/Z$ where $c_k(x)$ is the constant above, and the max of the softmax of the output heads of each of the OOD classes is taken. Then we divide everything by $Z$ which is a normalizing factor (this $max_j$ can be seen as the maximum of a softmax probability score across the heads of these OOD classes).
** Empirical Evaluation
*** Baselines
There are 12 baselines:
- Exemplar-free (does not save previous data):
  - HAT (argmax over the concatenated logits)
  - OWM
  - SLDA
  - L2P
- Replay Methods
  - iCaRL
  - A-GEM
  - EEIL
  - GD
  - DER++
  - HAL
  - MORE

There are no parameter-isolation methods included, nor contrastive learning methods
*** Datasets
1. CIFAR10 (10 different classes)
2. CIFAR100 (100 classes)
3. Tiny-ImageNet (200 classes)
*** Backbone Architecture and Pre-Training
The backbone selected is DeiT-S/16 (transformer). Also the classes are separated out when doing the training and testing of the models. The models used are not trained on ImageNet, or the training datasets to avoid leak in the model. Additionally, there is an *adapter module* appended to the end of the models to allow it to adjust to new data.
*** Training details
The classes in each of the datasets are split, and a memory budget strategy is used to save an equal number of samples per class.
*** Evaluation Metrocs
Two metrics used
1. Average Classification Accuracy (AUC)
2. Average Forgetting Rate (ACA): The accuracy of the model on all the tasks that it was previously trained on (more formally $F_t = \sum_{i=1}^{t-1} A_i^i-A_i^t$)

*** 5.1 Results and Comparison
The new method performs the best everywhere (and MORE is also a performant algorithm). Additioanlly, the size of the memory buffer can be decreased and performance is still maintained. Additionally, the forgetting rate is better than most, except for iCaRl, which has much lower accuracy. Additionally, the forgetting here is not due to the TIL model (HAT is being used which has very low forgetting), but the classification issues that happen with more classes


[[file:images/learnability-clp5.png]]
*** 5.2 Abalation Experiments
Performance decreases are clear.

[[file:images/learnability-clp6.png]]
** Conclusion
CIL is learnable, and ROW outperforms strong baselines
